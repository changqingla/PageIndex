# ä½¿ç”¨ Ollama æœ¬åœ°è¿è¡Œ PageIndexï¼ˆ5 åˆ†é’Ÿå¿«é€Ÿå¼€å§‹ï¼‰

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ åœ¨ 5 åˆ†é’Ÿå†…ä½¿ç”¨ Ollama æœ¬åœ°è¿è¡Œ PageIndexï¼Œå®Œå…¨å…è´¹ï¼Œæ— éœ€ API å¯†é’¥ã€‚

## å‰ç½®æ¡ä»¶

- è‡³å°‘ 16GB RAMï¼ˆæŽ¨è 32GB ç”¨äºŽ 70B æ¨¡åž‹ï¼‰
- 50GB å¯ç”¨ç£ç›˜ç©ºé—´

## æ­¥éª¤ 1ï¼šå®‰è£… Ollamaï¼ˆ1 åˆ†é’Ÿï¼‰

### macOS / Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows

ä¸‹è½½å¹¶å®‰è£…ï¼š[ollama.com/download](https://ollama.com/download)

éªŒè¯å®‰è£…ï¼š

```bash
ollama --version
```

## æ­¥éª¤ 2ï¼šä¸‹è½½æ¨¡åž‹ï¼ˆ2-5 åˆ†é’Ÿï¼‰

### æŽ¨èæ¨¡åž‹é€‰æ‹©

| æ¨¡åž‹ | å¤§å° | RAM éœ€æ±‚ | è¯­è¨€æ”¯æŒ | é€Ÿåº¦ | é€‚åˆåœºæ™¯ |
|------|------|---------|----------|------|---------|
| `qwen2.5:72b` | 41GB | 48GB | ä¸­è‹±æ–‡ | æ…¢ | æœ€ä½³å‡†ç¡®åº¦ |
| `qwen2.5:32b` | 19GB | 24GB | ä¸­è‹±æ–‡ | ä¸­ | å¹³è¡¡æ€§èƒ½ |
| `qwen2.5:14b` | 9GB | 16GB | ä¸­è‹±æ–‡ | å¿« | æ—¥å¸¸ä½¿ç”¨ |
| `llama3.1:70b` | 40GB | 48GB | è‹±æ–‡ | æ…¢ | è‹±æ–‡æ–‡æ¡£ |
| `llama3.1:8b` | 4.7GB | 8GB | è‹±æ–‡ | å¾ˆå¿« | æµ‹è¯•/æ¼”ç¤º |

### ä¸‹è½½æ¨¡åž‹

```bash
# æŽ¨èï¼šQwen2.5 32Bï¼ˆä¸­è‹±æ–‡æ··åˆæ–‡æ¡£ï¼‰
ollama pull qwen2.5:32b

# æˆ–è€…ï¼šQwen2.5 72Bï¼ˆæœ€ä½³æ•ˆæžœï¼Œéœ€è¦æ›´å¤šå†…å­˜ï¼‰
ollama pull qwen2.5:72b

# æˆ–è€…ï¼šLlama 3.1 8Bï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
ollama pull llama3.1:8b
```

## æ­¥éª¤ 3ï¼šé…ç½® PageIndexï¼ˆ30 ç§’ï¼‰

```bash
cd PageIndex

# åˆ›å»º .env æ–‡ä»¶
cat > .env << 'EOF'
CHATGPT_API_KEY=ollama
OPENAI_API_BASE=http://localhost:11434/v1
EOF
```

## æ­¥éª¤ 4ï¼šæµ‹è¯•é…ç½®ï¼ˆ30 ç§’ï¼‰

```bash
# æµ‹è¯•è¿žæŽ¥
python3 scripts/test_custom_model.py --model qwen2.5:32b
```

åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š

```
âœ… All tests passed! You're ready to use PageIndex.
```

## æ­¥éª¤ 5ï¼šè¿è¡Œ PageIndexï¼ˆ1-5 åˆ†é’Ÿï¼‰

```bash
# åŸºç¡€ä½¿ç”¨
python3 run_pageindex.py \
    --pdf_path tests/pdfs/2023-annual-report.pdf \
    --model qwen2.5:32b

# å¸¦æ‘˜è¦ç”Ÿæˆ
python3 run_pageindex.py \
    --pdf_path your_document.pdf \
    --model qwen2.5:32b \
    --if-add-node-summary yes

# å®Œæ•´åŠŸèƒ½
python3 run_pageindex.py \
    --pdf_path your_document.pdf \
    --model qwen2.5:32b \
    --if-add-node-summary yes \
    --if-add-doc-description yes \
    --if-add-node-text yes
```

## æŸ¥çœ‹ç»“æžœ

```bash
# æŸ¥çœ‹ç”Ÿæˆçš„æ ‘ç»“æž„
cat results/your_document_structure.json

# æˆ–ä½¿ç”¨ Python æŸ¥çœ‹
python3 -c "
import json
with open('results/your_document_structure.json') as f:
    data = json.load(f)
    print(json.dumps(data, indent=2, ensure_ascii=False)[:2000])
"
```

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. ä½¿ç”¨ GPU åŠ é€Ÿ

Ollama ä¼šè‡ªåŠ¨ä½¿ç”¨ GPUï¼ˆNVIDIA/AMD/Apple Siliconï¼‰ã€‚æ£€æŸ¥ï¼š

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi  # NVIDIA GPU

# macOS ä¼šè‡ªåŠ¨ä½¿ç”¨ Metal
```

### 2. è°ƒæ•´å¹¶å‘æ•°

å¦‚æžœå†…å­˜å……è¶³ï¼Œå¯ä»¥åœ¨ `config.yaml` ä¸­è°ƒæ•´ï¼š

```yaml
max_token_num_each_node: 30000  # å¢žå¤§ä»¥å‡å°‘è°ƒç”¨æ¬¡æ•°
```

### 3. æ¨¡åž‹é‡åŒ–

Ollama é»˜è®¤ä½¿ç”¨ 4-bit é‡åŒ–ï¼Œå¯ä»¥å°è¯•ä¸åŒç²¾åº¦ï¼š

```bash
# Q4_0: æœ€å¿«ï¼Œå ç”¨æœ€å°‘å†…å­˜ï¼ˆé»˜è®¤ï¼‰
ollama pull qwen2.5:32b-q4_0

# Q8_0: æ›´é«˜ç²¾åº¦
ollama pull qwen2.5:32b-q8_0
```

## å¸¸è§é—®é¢˜

### Q: Ollama æœåŠ¡æ²¡æœ‰å¯åŠ¨ï¼Ÿ

```bash
# macOS/Linux - Ollama é€šå¸¸è‡ªåŠ¨å¯åŠ¨
ollama serve

# æˆ–æ£€æŸ¥æ˜¯å¦å·²è¿è¡Œ
ps aux | grep ollama
```

### Q: å†…å­˜ä¸è¶³ï¼Ÿ

ä½¿ç”¨æ›´å°çš„æ¨¡åž‹ï¼š

```bash
ollama pull qwen2.5:14b
# ç„¶åŽä½¿ç”¨ --model qwen2.5:14b
```

### Q: é€Ÿåº¦å¤ªæ…¢ï¼Ÿ

1. ä½¿ç”¨æ›´å°çš„æ¨¡åž‹ï¼ˆ14B æˆ– 8Bï¼‰
2. ç¡®ä¿ä½¿ç”¨ GPU
3. å‡å°‘ `--if-add-node-summary` ç­‰é€‰é¡¹
4. å…³é—­å…¶ä»–å ç”¨ GPU çš„ç¨‹åº

### Q: è¾“å‡ºè´¨é‡ä¸å¥½ï¼Ÿ

1. ä½¿ç”¨æ›´å¤§çš„æ¨¡åž‹ï¼ˆ32B æˆ– 72Bï¼‰
2. å¯¹äºŽä¸­æ–‡æ–‡æ¡£ï¼Œä½¿ç”¨ Qwen ç³»åˆ—
3. å¯¹äºŽè‹±æ–‡æ–‡æ¡£ï¼Œå¯ä»¥å°è¯• Llama 3.1

### Q: å¦‚ä½•æ›´æ–°æ¨¡åž‹ï¼Ÿ

```bash
ollama pull qwen2.5:32b  # ä¼šè‡ªåŠ¨æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬
```

## ä¸Ž OpenAI å¯¹æ¯”

| ç‰¹æ€§ | Ollama (æœ¬åœ°) | OpenAI API |
|------|--------------|-----------|
| æˆæœ¬ | å…è´¹ | æŒ‰ä½¿ç”¨ä»˜è´¹ |
| éšç§ | å®Œå…¨æœ¬åœ° | æ•°æ®ä¸Šä¼ åˆ°äº‘ç«¯ |
| é€Ÿåº¦ | å–å†³äºŽç¡¬ä»¶ | é€šå¸¸æ›´å¿« |
| å‡†ç¡®åº¦ | Qwen 72B â‰ˆ GPT-4 | GPT-4o æœ€ä½³ |
| ç½‘ç»œè¦æ±‚ | ä»…ä¸‹è½½æ¨¡åž‹æ—¶ | å§‹ç»ˆéœ€è¦ |

## è¿›é˜¶ä½¿ç”¨

### æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£

```bash
for pdf in docs/*.pdf; do
    python3 run_pageindex.py \
        --pdf_path "$pdf" \
        --model qwen2.5:32b \
        --if-add-node-summary yes
done
```

### è‡ªå®šä¹‰æ¨¡åž‹å‚æ•°

åˆ›å»º Modelfileï¼š

```bash
cat > Modelfile << 'EOF'
FROM qwen2.5:32b
PARAMETER temperature 0
PARAMETER top_p 0.9
PARAMETER num_ctx 32768
EOF

# åˆ›å»ºè‡ªå®šä¹‰æ¨¡åž‹
ollama create my-qwen -f Modelfile

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡åž‹
python3 run_pageindex.py --pdf_path doc.pdf --model my-qwen
```

### ç›‘æŽ§èµ„æºä½¿ç”¨

```bash
# ç›‘æŽ§ GPU
watch -n 1 nvidia-smi

# ç›‘æŽ§å†…å­˜
htop
```

## ä¸‹ä¸€æ­¥

- ðŸ“– é˜…è¯» [CUSTOM_MODELS.md](CUSTOM_MODELS.md) äº†è§£æ›´å¤šé…ç½®é€‰é¡¹
- ðŸ” æŸ¥çœ‹ [tutorials/tree-search/](../tutorials/tree-search/) å­¦ä¹ å¦‚ä½•ä½¿ç”¨ç”Ÿæˆçš„æ ‘ç»“æž„è¿›è¡Œæ£€ç´¢
- ðŸ’¬ åŠ å…¥ [Discord ç¤¾åŒº](https://discord.com/invite/VuXuf29EUj) èŽ·å–å¸®åŠ©

## æ•…éšœæŽ’é™¤

å¦‚é‡åˆ°é—®é¢˜ï¼š

1. è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š`python3 scripts/test_custom_model.py --model qwen2.5:32b`
2. æ£€æŸ¥æ—¥å¿—ï¼š`ls logs/` æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
3. æŸ¥çœ‹ Ollama æ—¥å¿—ï¼š`journalctl -u ollama -f` (Linux) æˆ– `/var/log/ollama/ollama.log`
4. è®¿é—® [GitHub Issues](https://github.com/VectifyAI/PageIndex/issues)

## å®Œæ•´ç¤ºä¾‹è¾“å‡º

```json
{
  "doc_name": "2023-annual-report.pdf",
  "structure": [
    {
      "title": "Overview",
      "node_id": "0001",
      "start_index": 1,
      "end_index": 5,
      "summary": "This section provides an overview of the company's performance in 2023..."
    },
    {
      "title": "Financial Results",
      "node_id": "0002",
      "start_index": 6,
      "end_index": 15,
      "nodes": [
        {
          "title": "Revenue Analysis",
          "node_id": "0003",
          "start_index": 6,
          "end_index": 10,
          "summary": "Revenue increased by 23% year-over-year..."
        }
      ]
    }
  ]
}
```

---

ðŸŽ‰ æ­å–œï¼ä½ å·²ç»æˆåŠŸåœ¨æœ¬åœ°è¿è¡Œ PageIndexï¼
